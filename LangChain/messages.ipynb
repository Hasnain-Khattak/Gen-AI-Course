{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Messages**\n",
    "\n",
    "First let's talk about messages in LangChain.\n",
    "\n",
    "**In LangChain, messages are used to represent interactions between a user and an AI model. They are typically organized in a conversational format and are fundamental to building and managing context in chat-based or conversational AI applications.**\n",
    "\n",
    "> ### Now let's take a look on what LangChain say about messages.\n",
    "\n",
    "> Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
    ">\n",
    "> Each message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\n",
    ">\n",
    "> LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is inside a message?\n",
    "A message typically consists of the following pieces of information:\n",
    "\n",
    "- Role: The role of the message (e.g., \"user\", \"assistant\").\n",
    "- Content: The content of the message (e.g., text, multimodal data).\n",
    "- Additional metadata: id, name, token usage and other model-specific metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Types of Messages**\n",
    "\n",
    "LangChain categorizes messages into the following types:\n",
    "\n",
    "1. SystemMessage: for content which should be passed to direct the conversation\n",
    "2. HumanMessage: for content in the input from the user.\n",
    "3. AIMessage: for content in the response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note**\n",
    "\n",
    "In this Project I am using `ChatModel` name `ChatGroq`. We will discusses about it latter in other lecture, but right now I am using it to get the answer from llm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-groq -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HumanMessage**\n",
    "\n",
    "The `HumanMessage` corresponds to the \"user\" role. A human message represents input from a user interacting with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is an open-source Python library for building conversational AI applications. Messages in LangChain refer to a fundamental data structure used to represent interactions between a user and a conversational model.\\n\\nIn LangChain, a message is a dictionary that contains the following key-value pairs:\\n\\n- `text`: The text that the user inputted or the model generated.\\n- `user_input`: The user\\'s input. This is a boolean flag that is `True` if the message is a user\\'s input, and `False` otherwise.\\n- `model_output`: The model\\'s response to the user\\'s input. This is a boolean flag that is `True` if the message is the model\\'s output, and `False` otherwise.\\n- `prev_message`: The previous message in the conversation.\\n- `next_message`: The next message in the conversation.\\n\\nHere\\'s an example of what a message might look like in LangChain:\\n\\n```python\\nmsg = {\\n    \"text\": \"Hello, how are you?\",\\n    \"user_input\": True,\\n    \"model_output\": False,\\n    \"prev_message\": None,\\n    \"next_message\": None\\n}\\n```\\n\\nMessages are used throughout LangChain to build conversational flows, represent user interactions, and generate responses from models.\\n\\nHere are some possible scenarios where messages are used in LangChain:\\n\\n- Handling user input: When a user sends a message, it is stored as a message object in the LangChain model. This allows the model to keep track of the conversation history and respond accordingly.\\n- Generating model responses: When a model generates a response, it is stored as a message object. This allows the model to keep track of its own output and use it to inform future responses.\\n- Building conversational flows: Messages are used to construct conversational flows, which are sequences of user input and model responses. These flows can be used to implement complex conversational logic, such as handling user queries or providing recommendations.\\n\\nIn summary, messages in LangChain are a fundamental data structure that represents interactions between users and conversational models. They are used to build conversational flows, handle user input, and generate model responses.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 434, 'prompt_tokens': 42, 'total_tokens': 476, 'completion_time': 0.578666667, 'prompt_time': 0.003713152, 'queue_time': 0.051831014, 'total_time': 0.582379819}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c5e685ab6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-9bd76f73-9b0e-4fdf-8349-c8f38fbf8572-0', usage_metadata={'input_tokens': 42, 'output_tokens': 434, 'total_tokens': 476})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(api_key=\"Your-Groq-APIKEY\",\n",
    "                 model='llama-3.1-8b-instant') # Here we have to initialized our llm\n",
    "\n",
    "model.invoke([HumanMessage(content='What are messages in LangChain.')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain Tip**\n",
    "\n",
    "> When invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SystemMessage**\n",
    "\n",
    "A `SystemMessage` is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**What is LangChain?**\\n\\nLangChain is an open-source framework for building conversational AI applications. It\\'s designed to make it easier to create conversational interfaces using a variety of AI models, including language models, expert systems, and knowledge graphs.\\n\\n**Key Features of LangChain**\\n\\n1. **Modular Architecture**: LangChain is built around a modular architecture, which allows developers to easily swap out different components (e.g., models, databases, etc.) to create custom conversational interfaces.\\n2. **Support for Multiple AI Models**: LangChain supports a wide range of AI models, including language models (e.g., transformers, LSTMs, etc.), expert systems, and knowledge graphs.\\n3. **Knowledge Graph Integration**: LangChain provides built-in support for knowledge graphs, which allows developers to store and retrieve complex knowledge structures.\\n4. **Conversational Flow Management**: LangChain provides tools for managing conversational flow, including dialogue state tracking, intent recognition, and response generation.\\n5. **Extensive Library of Pre-built Components**: LangChain has an extensive library of pre-built components, including models, databases, and other utilities, which can be used to accelerate development.\\n\\n**Getting Started with LangChain**\\n\\nIf you\\'re interested in getting started with LangChain, here are the basic steps:\\n\\n1. **Install LangChain**: You can install LangChain using pip: `pip install langchain`\\n2. **Explore the Documentation**: LangChain has an extensive documentation set, which covers everything from basic usage to advanced topics.\\n3. **Start Building**: Once you\\'ve installed LangChain and explored the documentation, you can start building your own conversational AI applications.\\n\\n**Example Use Case: Chatbot Development**\\n\\nHere\\'s an example use case for LangChain: building a chatbot that can answer user questions about a specific topic.\\n\\n```python\\nimport langchain\\n\\n# Create a knowledge graph\\nkg = langchain.KnowledgeGraph(\\n    name=\"my_kg\",\\n    schema={\"entities\": [\"Person\"], \"relations\": [\"worked_at\"]}\\n)\\n\\n# Define a conversational flow\\nflow = langchain.ConversationalFlow(\\n    name=\"my_flow\",\\n    prompts=[\"What is the name of the CEO of Google?\"],\\n    responses=[\"The CEO of Google is Sundar Pichai.\"]\\n)\\n\\n# Create a chatbot using the knowledge graph and conversational flow\\nchatbot = langchain.Chatbot(\\n    name=\"my_chatbot\",\\n    knowledge_graph=kg,\\n    conversational_flow=flow\\n)\\n\\n# Run the chatbot\\nchatbot.run()\\n```\\n\\nThis code snippet demonstrates how to create a chatbot that can answer user questions using a knowledge graph and conversational flow.\\n\\n**Conclusion**\\n\\nLangChain is a powerful framework for building conversational AI applications. Its modular architecture, support for multiple AI models, and knowledge graph integration make it an attractive choice for developers. With LangChain, you can create complex conversational interfaces that can engage users and provide valuable insights.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 603, 'prompt_tokens': 58, 'total_tokens': 661, 'completion_time': 0.804, 'prompt_time': 0.004616303, 'queue_time': 0.05254159, 'total_time': 0.808616303}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c5e685ab6f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3bc0c3a0-f99d-4798-90d3-c0426a47f30f-0', usage_metadata={'input_tokens': 58, 'output_tokens': 603, 'total_tokens': 661})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful assistant. And your job in to guide us learning LangChain Framework'),\n",
    "    HumanMessage(content=\"WHat us LangChain?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AIMessage**\n",
    "\n",
    "`AIMessage` is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" It provides a set of tools and libraries that make it easier to develop AI chatbots, virtual assistants, and other conversational interfaces.\\n\\nLangChain is built on top of Python and utilizes a modular architecture, making it flexible and scalable. It supports various AI models, including language models like LLaMA, BERT, and T5, as well as other types of models like intent detection and entity recognition models.\\n\\nSome of the key features of LangChain include:\\n\\n1. **Conversational Flow**: LangChain allows you to define conversational flows using a simple, modular architecture. This makes it easy to create complex conversational scenarios.\\n2. **Model Integration**: LangChain supports integration with various AI models, including language models, intent detection models, and entity recognition models.\\n3. **Data Management**: LangChain provides tools for managing conversational data, including data loading, preprocessing, and storage.\\n4. **Natural Language Processing (NLP)**: LangChain includes a range of NLP tools and libraries, including tokenization, part-of-speech tagging, named entity recognition, and more.\\n5. **User Interface**: LangChain provides a simple, intuitive interface for interacting with conversational AI applications.\\n\\nSome of the benefits of using LangChain include:\\n\\n1. **Rapid Development**: LangChain provides a set of pre-built tools and libraries that make it easier to develop conversational AI applications.\\n2. **Flexibility**: LangChain's modular architecture makes it easy to customize and extend the framework to suit your specific needs.\\n3. **Scalability**: LangChain is designed to handle large volumes of conversational data and can be scaled up to meet the demands of complex conversational scenarios.\\n\\nLangChain is particularly useful for building conversational AI applications in a variety of domains, including:\\n\\n1. **Customer Service**: LangChain can be used to build chatbots that provide customer support and answer frequently asked questions.\\n2. **Virtual Assistants**: LangChain can be used to build virtual assistants that can perform tasks such as scheduling appointments and making reservations.\\n3. **Language Translation**: LangChain can be used to build language translation systems that can translate text and speech in real-time.\\n4. **Content Generation**: LangChain can be used to build content generation systems that can create text, images, and videos on demand.\\n\\nOverall, LangChain is a powerful framework for building conversational AI applications, and its flexibility, scalability, and ease of use make it an attractive choice for developers and organizations looking to build complex conversational interfaces.\\n\\nWould you like to know more about LangChain or is there a specific aspect you'd like to explore further?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 533, 'prompt_tokens': 72, 'total_tokens': 605, 'completion_time': 0.710666667, 'prompt_time': 0.005575069, 'queue_time': 0.019766291, 'total_time': 0.716241736}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-41bdf667-ae95-4c9c-bdec-01ee6f523d93-0', usage_metadata={'input_tokens': 72, 'output_tokens': 533, 'total_tokens': 605})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful assistant. And your job in to guide us learning LangChain Framework'),\n",
    "    HumanMessage(content=\"WHat us LangChain?\"),\n",
    "    AIMessage(content=\"LangChain is an open-source framework for building conversational AI applications.\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Combining all of thses which our `ChatPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You a mathematics teacher and your name is {name}'),\n",
    "        HumanMessage(content='Can you solve initial value problem'),\n",
    "        AIMessage(content='Yeah sure I am a vey talented Professor or Mathematics'),\n",
    "        HumanMessage('{user_input}')\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke(\n",
    "    {\n",
    "        'name': 'Abid Ali Chodwan',\n",
    "        'user_input': 'Ok first tell me about calculas'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You a mathematics teacher and your name is {name}', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you solve initial value problem', additional_kwargs={}, response_metadata={}), AIMessage(content='Yeah sure I am a vey talented Professor or Mathematics', additional_kwargs={}, response_metadata={}), HumanMessage(content='{user_input}', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To solve an initial value problem, I'll need to know the specific details. Can you please provide the following information:\\n\\n1. The differential equation (either a first-order or higher-order differential equation).\\n2. The initial condition(s) that accompany the differential equation.\\n\\nOnce you provide the necessary information, I'll be happy to assist you in solving the initial value problem.\\n\\nFor example, if you have a first-order differential equation of the form:\\n\\ndy/dx = f(x,y)\\n\\nwith an initial condition of:\\n\\ny(x0) = y0\\n\\nPlease provide the function f(x,y) and the initial condition y(x0) = y0.\\n\\nI'll guide you through the solution using mathematical techniques such as separation of variables, integration factor, or numerical methods if necessary.\\n\\nNow, I am Aryan Prakash a mathematics teacher\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 77, 'total_tokens': 245, 'completion_time': 0.224, 'prompt_time': 0.006362636, 'queue_time': 0.016757174, 'total_time': 0.230362636}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-e08c20bc-b031-4e43-b814-36adcb4c9ac7-0', usage_metadata={'input_tokens': 77, 'output_tokens': 168, 'total_tokens': 245})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
