{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retriver Chain**\n",
    "\n",
    "The Retriever Chain in LangChain fetches relevant information from a data source to assist a language model in answering queries. It uses a retriever to search for relevant documents and may preprocess them before passing them to the model. This helps the model generate accurate and context-aware responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader('https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html#create-retrieval-chain')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GROQ_API_KEY'] = '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'\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyB4bdbCaHraBKMqmnjkqfr_CPlF3UKmU90\"     ## add your api key here\n",
    "os.environ['COHERE_API_KEY'] = \"k8PvagpN1xDFqXJ2mt7xbSrpNLPyigsSbLAiJNLh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "embadding = CohereEmbeddings(model=\"embed-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embadding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "retriver = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name='history'),\n",
    "        ('user', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_awar_chain = create_history_aware_retriever(model, retriver, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = [HumanMessage(content='What is create_retrieval_chain in langchain?'), AIMessage(content=\"Create retrieval chain that retrieves documents and then passes them on.\")]\n",
    "response = history_awar_chain.invoke({\n",
    "    'history': chat_history,\n",
    "    'input': 'What is ask you in privious message'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='428a6f40-afba-4861-a30e-1c9e104dd9a9', metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html#create-retrieval-chain', 'title': 'create_retrieval_chain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='Parameters:\\n\\nretriever (BaseRetriever | Runnable[dict, list[Document]]) â€“ Retriever-like object that returns list of documents. Should\\neither be a subclass of BaseRetriever or a Runnable that returns\\na list of documents. If a subclass of BaseRetriever, then it\\nis expected that an input key be passed in - this is what\\nis will be used to pass into the retriever. If this is NOT a\\nsubclass of BaseRetriever, then all the inputs will be passed\\ninto this runnable, meaning that runnable should take a dictionary\\nas input.\\ncombine_docs_chain (Runnable[Dict[str, Any], str]) â€“ Runnable that takes inputs and produces a string output.\\nThe inputs to this will be any original inputs to this chain, a new\\ncontext key with the retrieved documents, and chat_history (if not present\\nin the inputs) with a value of [] (to easily enable conversational\\nretrieval.\\n\\n\\nReturns:\\nAn LCEL Runnable. The Runnable return is a dictionary containing at the very\\nleast a context and answer key.\\n\\nReturn type:\\nRunnable\\n\\n\\nExample\\n# pip install -U langchain langchain-community\\n\\nfrom langchain_community.chat_models import ChatOpenAI\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain import hub\\n\\nretrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\\nllm = ChatOpenAI()\\nretriever = ...\\ncombine_docs_chain = create_stuff_documents_chain(\\n    llm, retrieval_qa_chat_prompt\\n)\\nretrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\\n\\nchain.invoke({\"input\": \"...\"})\\n\\n\\n\\nExamples using create_retrieval_chain\\n\\nApertureDB\\nBuild a PDF ingestion and Question/Answering system\\nBuild a Retrieval Augmented Generation (RAG) App\\nConversational RAG\\nHow to add chat history\\nHow to get your RAG application to return sources\\nHow to stream results from your RAG application\\nImage captions\\nJina Reranker\\nLoad docs\\nRAGatouille\\n\\n\\n\\n\\n\\n\\n\\n\\n On this page\\n  \\n\\n\\ncreate_retrieval_chain()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n      Â© Copyright 2023, LangChain Inc.'),\n",
       " Document(id='1ecaac83-9dff-44a5-8191-31426fee20a1', metadata={'source': 'https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html#create-retrieval-chain', 'title': 'create_retrieval_chain â€” ðŸ¦œðŸ”— LangChain  documentation', 'language': 'en'}, page_content='create_retrieval_chain â€” ðŸ¦œðŸ”— LangChain  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\nBack to top\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\nX / Twitter\\n\\n\\n\\n\\n\\n\\n\\nSection Navigation\\nBase packages\\n\\nCore\\nLangchain\\nagents\\ncallbacks\\nchains\\nChain\\nBaseCombineDocumentsChain\\nAsyncCombineDocsProtocol\\nCombineDocsProtocol\\nConstitutionalPrinciple\\nBaseConversationalRetrievalChain\\nChatVectorDBChain\\nInputType\\nElasticsearchDatabaseChain\\nFlareChain\\nQuestionGeneratorChain\\nFinishedOutputParser\\nHypotheticalDocumentEmbedder\\nOpenAIModerationChain\\nCrawler\\nElementInViewPort\\nFactWithEvidence\\nQuestionAnswer\\nSimpleRequestChain\\nAnswerWithSources\\nBasePromptSelector\\nConditionalPromptSelector\\nLoadingCallable\\nRetrievalQAWithSourcesChain\\nVectorDBQAWithSourcesChain\\nStructuredQueryOutputParser\\nISO8601Date\\nISO8601DateTime\\nAttributeInfo\\nLoadingCallable\\nMultiRouteChain\\nRoute\\nRouterChain\\nEmbeddingRouterChain\\nRouterOutputParser\\nMultiRetrievalQAChain\\nSequentialChain\\nSimpleSequentialChain\\nSQLInput\\nSQLInputWithTables\\nLoadingCallable\\nTransformChain\\nacollapse_docs\\ncollapse_docs\\nsplit_list_of_docs\\ncreate_stuff_documents_chain\\ngenerate_example\\ncreate_history_aware_retriever\\ncreate_citation_fuzzy_match_runnable\\nopenapi_spec_to_openai_fn\\nget_llm_kwargs\\nis_chat_model\\nis_llm\\nconstruct_examples\\nfix_filter_directive\\nget_query_constructor_prompt\\nload_query_constructor_runnable\\nget_parser\\nv_args\\ncreate_retrieval_chain\\ncreate_sql_query_chain\\nget_openai_output_parser\\nload_summarize_chain\\nAPIChain\\nAnalyzeDocumentChain\\nMapReduceDocumentsChain\\nMapRerankDocumentsChain\\nReduceDocumentsChain\\nRefineDocumentsChain\\nStuffDocumentsChain\\nConstitutionalChain\\nConversationChain\\nConversationalRetrievalChain\\nLLMChain\\nLLMCheckerChain\\nLLMMathChain\\nLLMSummarizationCheckerChain\\nMapReduceChain\\nNatBotChain\\nQAGenerationChain\\nBaseQAWithSourcesChain\\nQAWithSourcesChain\\nBaseRetrievalQA\\nRetrievalQA\\nVectorDBQA\\nLLMRouterChain\\nMultiPromptChain\\nload_chain\\nload_chain_from_config\\ncreate_openai_fn_chain\\ncreate_structured_output_chain\\ncreate_citation_fuzzy_match_chain\\ncreate_extraction_chain\\ncreate_extraction_chain_pydantic\\nget_openapi_chain\\ncreate_qa_with_sources_chain\\ncreate_qa_with_structure_chain\\ncreate_tagging_chain\\ncreate_tagging_chain_pydantic\\ncreate_extraction_chain_pydantic\\nload_qa_with_sources_chain\\nload_query_constructor_chain\\nload_qa_chain\\ncreate_openai_fn_runnable\\ncreate_structured_output_runnable\\n\\n\\nchat_models\\nembeddings\\nevaluation\\nglobals\\nhub\\nindexes\\nmemory\\nmodel_laboratory\\noutput_parsers\\nretrievers\\nrunnables\\nsmith\\nstorage\\n\\n\\nText Splitters\\nCommunity\\nExperimental\\n\\nIntegrations\\n\\nAI21\\nAnthropic\\nAstraDB\\nAWS\\nAzure Dynamic Sessions\\nCerebras\\nChroma\\nCohere\\nDatabricks\\nElasticsearch\\nExa\\nFireworks\\nGoogle Community\\nGoogle GenAI\\nGoogle VertexAI\\nGroq\\nHuggingface\\nIBM\\nMilvus\\nMistralAI\\nNeo4J\\nNomic\\nNvidia Ai Endpoints\\nOllama\\nOpenAI\\nPinecone\\nPostgres\\nPrompty\\nQdrant\\nRedis\\nSema4\\nSnowflake\\nSqlserver\\nStandard Tests\\nTogether\\nUnstructured\\nUpstage\\nVoyageAI\\nWeaviate\\nXAI\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain Python API Reference\\nlangchain: 0.3.15\\nchains\\ncreate_retrieval_chain\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncreate_retrieval_chain#\\n\\n\\nlangchain.chains.retrieval.create_retrieval_chain(retriever: BaseRetriever | Runnable[dict, list[Document]], combine_docs_chain: Runnable[Dict[str, Any], str]) â†’ Runnable[source]#\\nCreate retrieval chain that retrieves documents and then passes them on.\\n\\nParameters:')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embadding )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "retriver = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "prompt_1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'Answer the user query based on this context: \\n\\n {context}'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('user', '{input}')\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(model, prompt_1)\n",
    "retriever_chain = create_retrieval_chain(retriver, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [HumanMessage(content='What is create_retrieval_chain in langchain?'), AIMessage(content=\"Create retrieval chain that retrieves documents and then passes them on.\")]\n",
    "response = retriever_chain.invoke({\n",
    "    'chat_history': history,\n",
    "    'input': 'What is ask you in privious message'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me to explain what `create_retrieval_chain` is in the `langchain` library.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
