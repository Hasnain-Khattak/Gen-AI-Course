{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VectorStore**\n",
    "\n",
    "> A vectorstore is a storage backend for embeddings, allowing you to index, store, and query them efficiently. It pairs embeddings with metadata to support searches for semantically similar content.\n",
    "\n",
    "**Vectorstores in LangChain are specialized tools designed to store and retrieve vector embeddings for text or other data, which are essential for tasks like similarity search, question answering, and RAG (Retrieval-Augmented Generation) systems.**\n",
    "\n",
    "## Popular Vectorstore Options in LangChain\n",
    "\n",
    "1. FAISS (Facebook AI Similarity Search)\n",
    "2. Pinecone\n",
    "3. Chroma\n",
    "4. Qdrant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Embaddings we will use 3 different sources of embaddings\n",
    "\n",
    "1. GoogleGenerativeAIEmbeddings\n",
    "2. Cohere\n",
    "3. HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FAISS**\n",
    "\n",
    "Facebook AI Similarity Search (FAISS) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-google-genai langchain-cohere -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "\n",
    "file_path = './Data/RAG_Types_Table.docx'\n",
    "\n",
    "loader = Docx2txtLoader(file_path)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/RAG_Types_Table.docx'}, page_content='RAG Types: Advantages, Disadvantages, Use Cases, and Additional Information\\n\\nRAG Type\\n\\nAdvantages\\n\\nDisadvantages\\n\\nWhen to Use\\n\\nAdditional Information\\n\\nHybrid RAG\\n\\n- High accuracy by combining multiple information sources\\n- Handles diverse types of data (structured, unstructured) well\\n- Robust in challenging scenarios\\n\\n- Complexity in implementation\\n- Higher computational resources required\\n- Increased latency\\n\\n- When accuracy is paramount, and there are multiple data types\\n\\nCombines retrieval-based techniques (like search engines or databases) and generation-based techniques (like GPT-based models) to provide comprehensive responses.\\n\\nGenerative RAG\\n\\n- Provides flexible and creative responses\\n- Can generate human-like content\\n- Capable of handling open-domain questions\\n\\n- Risk of generating hallucinated information\\n- Requires more extensive training data\\n\\n- For open-ended or creative tasks, generating human-like answers\\n\\nFocuses more on generative approaches by leveraging large language models (LLMs) to generate answers, which may include context-based information fetched from external sources.\\n\\nRetrieval RAG\\n\\n- Provides precise, contextually relevant information\\n- Efficiently scales with large data\\n- Suitable for factual accuracy\\n\\n- Limited flexibility in response generation\\n- May not adapt well to highly abstract or creative tasks\\n\\n- When factual correctness is a priority\\n\\nPrimarily relies on robust retrieval systems such as search engines or database queries to provide pre-fetched content that is more factual and contextually accurate.\\n\\nKnowledge-based RAG\\n\\n- Integrates domain-specific knowledge\\n- High reliability in specific domains\\n- Supports real-time information and knowledge updates\\n\\n- Limited to the knowledge base updates\\n- May not perform well in general contexts\\n\\n- When domain-specific knowledge is required\\n\\nUses a structured knowledge base to answer questions, making it ideal for use cases like medical consultations, technical troubleshooting, and specialized customer support.\\n\\nEnd-to-End RAG\\n\\n- Seamless integration of retrieval and generation\\n- Provides a unified framework\\n- Can be fine-tuned for specific applications\\n\\n- Difficult to debug errors in responses\\n- High model complexity\\n\\n- When there is a need for streamlined, single-model architecture\\n\\nThese RAG systems combine both retrieval and generation processes in a tightly integrated end-to-end model, making them suitable for applications requiring high cohesion between information retrieval and generation.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "os.environ['Google_API_KEY'] = \"You-API-KEY\"\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x25b38251460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ques = 'Types of RAG?'\n",
    "\n",
    "answer = db.similarity_search(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e11a7630-8931-45c9-98a3-3447355fb529', metadata={'source': './Data/RAG_Types_Table.docx'}, page_content='RAG Types: Advantages, Disadvantages, Use Cases, and Additional Information\\n\\nRAG Type\\n\\nAdvantages\\n\\nDisadvantages\\n\\nWhen to Use\\n\\nAdditional Information\\n\\nHybrid RAG\\n\\n- High accuracy by combining multiple information sources\\n- Handles diverse types of data (structured, unstructured) well\\n- Robust in challenging scenarios\\n\\n- Complexity in implementation\\n- Higher computational resources required\\n- Increased latency\\n\\n- When accuracy is paramount, and there are multiple data types\\n\\nCombines retrieval-based techniques (like search engines or databases) and generation-based techniques (like GPT-based models) to provide comprehensive responses.\\n\\nGenerative RAG\\n\\n- Provides flexible and creative responses\\n- Can generate human-like content\\n- Capable of handling open-domain questions\\n\\n- Risk of generating hallucinated information\\n- Requires more extensive training data\\n\\n- For open-ended or creative tasks, generating human-like answers\\n\\nFocuses more on generative approaches by leveraging large language models (LLMs) to generate answers, which may include context-based information fetched from external sources.\\n\\nRetrieval RAG\\n\\n- Provides precise, contextually relevant information\\n- Efficiently scales with large data\\n- Suitable for factual accuracy\\n\\n- Limited flexibility in response generation\\n- May not adapt well to highly abstract or creative tasks\\n\\n- When factual correctness is a priority\\n\\nPrimarily relies on robust retrieval systems such as search engines or database queries to provide pre-fetched content that is more factual and contextually accurate.\\n\\nKnowledge-based RAG\\n\\n- Integrates domain-specific knowledge\\n- High reliability in specific domains\\n- Supports real-time information and knowledge updates\\n\\n- Limited to the knowledge base updates\\n- May not perform well in general contexts\\n\\n- When domain-specific knowledge is required\\n\\nUses a structured knowledge base to answer questions, making it ideal for use cases like medical consultations, technical troubleshooting, and specialized customer support.\\n\\nEnd-to-End RAG\\n\\n- Seamless integration of retrieval and generation\\n- Provides a unified framework\\n- Can be fine-tuned for specific applications\\n\\n- Difficult to debug errors in responses\\n- High model complexity\\n\\n- When there is a need for streamlined, single-model architecture\\n\\nThese RAG systems combine both retrieval and generation processes in a tightly integrated end-to-end model, making them suitable for applications requiring high cohesion between information retrieval and generation.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Qdrant**\n",
    "\n",
    "Qdrant (read: quadrant ) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage vectors with additional payload and extended filtering support. It makes it useful for all sorts of neural network or semantic-based matching, faceted search, and other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/RAG_Types_Table.docx'}, page_content='RAG Types: Advantages, Disadvantages, Use Cases, and Additional Information\\n\\nRAG Type\\n\\nAdvantages\\n\\nDisadvantages\\n\\nWhen to Use\\n\\nAdditional Information\\n\\nHybrid RAG\\n\\n- High accuracy by combining multiple information sources\\n- Handles diverse types of data (structured, unstructured) well\\n- Robust in challenging scenarios\\n\\n- Complexity in implementation\\n- Higher computational resources required\\n- Increased latency\\n\\n- When accuracy is paramount, and there are multiple data types\\n\\nCombines retrieval-based techniques (like search engines or databases) and generation-based techniques (like GPT-based models) to provide comprehensive responses.\\n\\nGenerative RAG\\n\\n- Provides flexible and creative responses\\n- Can generate human-like content\\n- Capable of handling open-domain questions\\n\\n- Risk of generating hallucinated information\\n- Requires more extensive training data\\n\\n- For open-ended or creative tasks, generating human-like answers\\n\\nFocuses more on generative approaches by leveraging large language models (LLMs) to generate answers, which may include context-based information fetched from external sources.\\n\\nRetrieval RAG\\n\\n- Provides precise, contextually relevant information\\n- Efficiently scales with large data\\n- Suitable for factual accuracy\\n\\n- Limited flexibility in response generation\\n- May not adapt well to highly abstract or creative tasks\\n\\n- When factual correctness is a priority\\n\\nPrimarily relies on robust retrieval systems such as search engines or database queries to provide pre-fetched content that is more factual and contextually accurate.\\n\\nKnowledge-based RAG\\n\\n- Integrates domain-specific knowledge\\n- High reliability in specific domains\\n- Supports real-time information and knowledge updates\\n\\n- Limited to the knowledge base updates\\n- May not perform well in general contexts\\n\\n- When domain-specific knowledge is required\\n\\nUses a structured knowledge base to answer questions, making it ideal for use cases like medical consultations, technical troubleshooting, and specialized customer support.\\n\\nEnd-to-End RAG\\n\\n- Seamless integration of retrieval and generation\\n- Provides a unified framework\\n- Can be fine-tuned for specific applications\\n\\n- Difficult to debug errors in responses\\n- High model complexity\\n\\n- When there is a need for streamlined, single-model architecture\\n\\nThese RAG systems combine both retrieval and generation processes in a tightly integrated end-to-end model, making them suitable for applications requiring high cohesion between information retrieval and generation.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_key = \"You-API-KEY\"\n",
    "qdrant_url = \"You-Qdrant-URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./Data/Central_Limit_Theorem.pdf')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 0}, page_content='Bernoulli distribution is a probability distribution that models a binary outcome, where the \\noutcome can be either success (represented by the value 1) or failure (represented by the \\nvalue 0). The Bernoulli distribution is named after the Swiss mathematician Jacob Bernoulli, \\nwho first introduced it in the late 1600s.\\nThe Bernoulli distribution is characterized by a single parameter, which is the probability of \\nsuccess, denoted by p. The probability mass function (PMF) of the Bernoulli distribution is:\\nThe Bernoulli distribution is commonly used in machine learning for modelling \\nbinary outcomes, such as whether a customer will make a purchase or not, \\nwhether an email is spam or not, or whether a patient will have a certain disease \\nor not.\\nBernoulli Distribution\\n27 March 2023 16:06\\n   Session on Central Limit Theorem Page 1    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 1}, page_content='Criteria:\\nBinomial distribution is a probability distribution that describes the number of \\nsuccesses in a fixed number of independent Bernoulli trials with two possible \\noutcomes (often called \"success\" and \"failure\"), where the probability of success \\nis constant for each trial. The binomial distribution is characterized by two \\nparameters: the number of trials n and the probability of success p.\\nThe Probability of anyone watching this lecture in the future and then liking it is 0.5. What is the \\nprobability that:\\nNo-one out of 3 people will like it1.\\n1 out of 3 people will like it1.\\n2 out of 3 people will like it1.\\n3 out of 3 people will like it1.\\nPDF Formula:\\nGraph of PDF:\\nBinomial Distribution\\n27 March 2023 16:36\\n   Session on Central Limit Theorem Page 2    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 2}, page_content='Criteria:\\nThe process consists of n trials1.\\nOnly 2 exclusive outcomes are possible, a success and a failure.2.\\nP(success) = p and P(failure) = 1-p and it is fixed from trial to trial3.\\nThe trials are independent.4.\\nBinary classification problems: In binary classification problems, we often model \\nthe probability of an event happening as a binomial distribution. For example, in a \\nspam detection system, we may model the probability of an email being spam or \\nnot spam using a binomial distribution.\\n1.\\nHypothesis testing: In statistical hypothesis testing, we use the binomial \\ndistribution to calculate the probability of observing a certain number of \\nsuccesses in a given number of trials, assuming a null hypothesis is true. This can \\nbe used to make decisions about whether a certain hypothesis is supported by \\nthe data or not.\\n2.\\nLogistic regression: Logistic regression is a popular machine learning algorithm \\nused for classification problems. It models the probability of an event happening \\nas a logistic function of the input variables. Since the logistic function can be \\nviewed as a transformation of a linear combination of inputs, the output of \\nlogistic regression can be thought of as a binomial distribution.\\n3.\\nA/B testing: A/B testing is a common technique used to compare two different \\nversions of a product, web page, or marketing campaign. In A/B testing, we \\nrandomly assign individuals to one of two groups and compare the outcomes of \\ninterest between the groups. Since the outcomes are often binary (e.g., click-\\nthrough rate or conversion rate), the binomial distribution can be used to model \\nthe distribution of outcomes and test for differences between the groups.\\n4.\\n   Session on Central Limit Theorem Page 3    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 3}, page_content='Sampling distribution is a probability distribution that describes the statistical properties of a \\nsample statistic (such as the sample mean or sample proportion) computed from multiple \\nindependent samples of the same size from a population.\\nWhy Sampling Distribution is important?\\nSampling distribution is important in statistics and machine learning because it allows us to \\nestimate the variability of a sample statistic, which is useful for making inferences about the \\npopulation. By analysing the properties of the sampling distribution, we can compute \\nconfidence intervals, perform hypothesis tests, and make predictions about the population \\nbased on the sample data.\\nSampling Distribution\\n27 March 2023 17:10\\n   Session on Central Limit Theorem Page 4    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 4}, page_content='The Central Limit Theorem (CLT) states that the distribution of the sample means of a large \\nnumber of independent and identically distributed random variables will approach a normal \\ndistribution, regardless of the underlying distribution of the variables.\\nThe conditions required for the CLT to hold are:\\nThe sample size is large enough, typically greater than or equal to 30.1.\\nThe sample is drawn from a finite population or an infinite population with a finite \\nvariance.\\n2.\\nThe random variables in the sample are independent and identically distributed.3.\\nThe CLT is important in statistics and machine learning because it allows us to \\nmake probabilistic inferences about a population based on a sample of data. For \\nexample, we can use the CLT to construct confidence intervals, perform \\nhypothesis tests, and make predictions about the population mean based on the \\nsample data. The CLT also provides a theoretical justification for many commonly \\nused statistical techniques, such as t-tests, ANOVA, and linear regression.\\nCentral Limit Theorem\\n27 March 2023 17:10\\n   Session on Central Limit Theorem Page 5    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 5}, page_content='Case Study 1 - Titanic Fare\\n28 March 2023 17:19\\n   Session on Central Limit Theorem Page 6    '),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 6}, page_content=\"Step-by-step process:\\nCollect multiple random samples of salaries from a representative group \\nof Indians. Each sample should be large enough (usually, n > 30) to ensure \\nthe CLT holds. Make sure the samples are representative and unbiased to \\navoid skewed results.\\n1.\\nCalculate the sample mean (average salary) and sample standard \\ndeviation for each sample.\\n2.\\nCalculate the average of the sample means. This value will be your best \\nestimate of the population mean (average salary of all Indians).\\n3.\\nCalculate the standard error of the sample means, which is the standard \\ndeviation of the sample means divided by the square root of the number \\nof samples.\\n4.\\nCalculate the confidence interval around the average of the sample means \\nto get a range within which the true population mean likely falls. For a \\n95% confidence interval:\\n5.\\nlower_limit = average_sample_means - 1.96 * standard_error \\nupper_limit = average_sample_means + 1.96 * standard_error\\nReport the estimated average salary and the confidence interval.6.\\nPython code\\nRemember that the validity of your results depends on the quality of your \\ndata and the representativeness of your samples. To obtain accurate \\nresults, it's crucial to ensure that your samples are unbiased and \\nrepresentative.\\nCase Study - What is the average income of Indians\\n28 March 2023 15:49\\n   Session on Central Limit Theorem Page 7    \")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=50)\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "embeddings = CohereEmbeddings(cohere_api_key=\"You-API-KEY\", model=\"embed-english-v3.0\")\n",
    "\n",
    "batch_size = 100  # Adjust based on your dataset size\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    Qdrant.from_documents(\n",
    "        batch, embeddings, url=qdrant_url, api_key=qdrant_key, collection_name='statistics'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=70 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfig(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, filter_max_conditions=None, condition_max_size=None)) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Connect to the Qdrant server\n",
    "client = QdrantClient(url=qdrant_url, api_key=qdrant_key)\n",
    "\n",
    "# Check collection info\n",
    "collection_info = client.get_collection(collection_name=\"statistics\")\n",
    "print(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example search query\n",
    "query = \"Find statistics related documents\"\n",
    "\n",
    "# Generate the embedding for the query\n",
    "query_embedding = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_11964\\112132168.py:2: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# Perform the similarity search\n",
    "search_results = client.search(\n",
    "    collection_name=\"statistics\",\n",
    "    query_vector=query_embedding,  # The query embedding\n",
    "    limit=5,  # Number of top results to return\n",
    ")\n",
    "\n",
    "# Display search results\n",
    "for result in search_results:\n",
    "    print(f\"Document ID: {result.id}, Score: {result.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: b038f7ff-308f-4bac-ae5b-3262c71b0f4c\n",
      "Page Content: avoid skewed results.\n",
      "1.\n",
      "Calculate the sample mean (average salary) and sample standard \n",
      "deviation for each sample.\n",
      "2.\n",
      "Metadata: {'source': './Data/Central_Limit_Theorem.pdf', 'page': 6}\n",
      "--------------------------------------------------\n",
      "Document ID: b6ef766e-1f7d-4649-bcba-cca95f2cc4d0\n",
      "Page Content: Why Sampling Distribution is important?\n",
      "Sampling distribution is important in statistics and machine learning because it allows us to\n",
      "Metadata: {'source': './Data/Central_Limit_Theorem.pdf', 'page': 3}\n",
      "--------------------------------------------------\n",
      "Document ID: 8e070619-6116-478c-99c4-58db4fb9ea61\n",
      "Page Content: sample statistic (such as the sample mean or sample proportion) computed from multiple \n",
      "independent samples of the same size from a population.\n",
      "Metadata: {'source': './Data/Central_Limit_Theorem.pdf', 'page': 3}\n",
      "--------------------------------------------------\n",
      "Document ID: 4819ccd9-d110-4647-be8f-b3ccd2bb6d87\n",
      "Page Content: estimate the variability of a sample statistic, which is useful for making inferences about the\n",
      "Metadata: {'source': './Data/Central_Limit_Theorem.pdf', 'page': 3}\n",
      "--------------------------------------------------\n",
      "Document ID: d9c8abc5-fadb-49f9-84c4-74840753211c\n",
      "Page Content: not spam using a binomial distribution.\n",
      "1.\n",
      "Hypothesis testing: In statistical hypothesis testing, we use the binomial\n",
      "Metadata: {'source': './Data/Central_Limit_Theorem.pdf', 'page': 2}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop through the search results\n",
    "for result in search_results:\n",
    "    # Extract document ID\n",
    "    document_id = result.id\n",
    "    \n",
    "    # Extract page content and metadata\n",
    "    page_content = result.payload.get(\"page_content\", \"No page content available\")\n",
    "    metadata = result.payload.get(\"metadata\", {})\n",
    "    \n",
    "    # Display the information\n",
    "    print(f\"Document ID: {document_id}\")\n",
    "    print(f\"Page Content: {page_content}\")\n",
    "    print(f\"Metadata: {metadata}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now saving the vector store in out local pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_db = Qdrant.from_documents(\n",
    "    chunks, embeddings, path='vectore_store', collection_name='Statistics'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 6, '_id': '83d11b7152db41a79a2ca1bb57f41c4a', '_collection_name': 'Statistics'}, page_content='avoid skewed results.\\n1.\\nCalculate the sample mean (average salary) and sample standard \\ndeviation for each sample.\\n2.'),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 3, '_id': '795db2b04ffb4905bbb5399d2c6a446f', '_collection_name': 'Statistics'}, page_content='Why Sampling Distribution is important?\\nSampling distribution is important in statistics and machine learning because it allows us to'),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 3, '_id': '51eef85e4e464ee2aac6ecd17b693971', '_collection_name': 'Statistics'}, page_content='sample statistic (such as the sample mean or sample proportion) computed from multiple \\nindependent samples of the same size from a population.'),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 3, '_id': '021fe48cab2843adbbf597551f9e59bc', '_collection_name': 'Statistics'}, page_content='estimate the variability of a sample statistic, which is useful for making inferences about the')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = vectore_db.similarity_search('Find statistics related documents')\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectore_db.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs= {'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_11964\\3253713736.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriver.get_relevant_documents('Find statistics related documents')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 6, '_id': '83d11b7152db41a79a2ca1bb57f41c4a', '_collection_name': 'Statistics'}, page_content='avoid skewed results.\\n1.\\nCalculate the sample mean (average salary) and sample standard \\ndeviation for each sample.\\n2.'),\n",
       " Document(metadata={'source': './Data/Central_Limit_Theorem.pdf', 'page': 4, '_id': '8dff57da01264b7bb0b798037092fb55', '_collection_name': 'Statistics'}, page_content='The CLT is important in statistics and machine learning because it allows us to')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.get_relevant_documents('Find statistics related documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
