{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chains**\n",
    "\n",
    "**In LangChain, chains refer to sequences of actions that combine multiple components or tasks into a structured pipeline. They allow developers to build complex workflows by linking together various operations, such as querying a language model, processing its output, retrieving information from external sources, and interacting with APIs or databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chains are easily reusable components linked together.\n",
    ">\n",
    "> Chains encode a sequence of calls to components like models, document retrievers, other Chains, etc., and provide a simple interface to this sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLMChain**\n",
    "\n",
    "LLMChain combined a prompt template, LLM, and output parser into a class.\n",
    "\n",
    "Some advantages of switching to the LCEL implementation are:\n",
    "\n",
    "Clarity around contents and parameters. The legacy LLMChain contains a default output parser and other options.\n",
    "\n",
    "Easier streaming. LLMChain only supports streaming via callbacks.\n",
    "Easier access to raw message outputs if desired. LLMChain only exposes these via a parameter or via callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_13128\\1460545614.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  Legacy_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('user', 'Tell me a joke about {input}')\n",
    "])\n",
    "\n",
    "Legacy_chain = LLMChain(\n",
    "    llm=ChatGroq(api_key=\"You-API-Key\",\n",
    "                 model='llama-3.1-8b-instant',\n",
    "                 temperature=0.2),\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'car',\n",
       " 'text': 'Why did the car go to the doctor?\\n\\nBecause it was feeling a little \"car-sick\"!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legacy_result = Legacy_chain({\"input\": \"car\"})\n",
    "legacy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Note`** that LLMChain by default returned a dict containing both the input and the output from StrOutputParser, so to extract the output, you need to access the \"text\" key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the car go to the doctor?\\n\\nBecause it was feeling a little \"car-sick\"!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legacy_result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('user', 'Tell me a joke about {input}')\n",
    "])\n",
    "\n",
    "chain = prompt | ChatGroq(api_key=\"You-API-Key\",\n",
    "                 model='llama-3.1-8b-instant',\n",
    "                 temperature=0.2) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the house go to therapy? \\n\\nBecause it had a lot of \"foundation\" issues.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"house\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to mimic the `dict` packaging of input and output in `LLMChain`, you can use a RunnablePassthrough.assign like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'candy',\n",
       " 'text': 'Why did the lollipop go to the party? \\n\\nBecause it was a sucker for fun.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "outer_chain = RunnablePassthrough().assign(text=chain)\n",
    "\n",
    "outer_chain.invoke({\"input\": \"candy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **ConversationChain**\n",
    "ConversationChain incorporated a memory of previous messages to sustain a stateful conversation.\n",
    "\n",
    "Some advantages of switching to the Langgraph implementation are:\n",
    "\n",
    "- Innate support for threads/separate sessions. To make this work with `ConversationChain`, you'd need to instantiate a separate memory class outside the chain.\n",
    "- More explicit parameters. `ConversationChain` contains a hidden default prompt, which can cause confusion.\n",
    "- Streaming support. `ConversationChain` only supports streaming via callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_13128\\4010190727.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_13128\\4010190727.py:16: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm Lava, how are you?\",\n",
       " 'history': '',\n",
       " 'response': 'Arrr, I be doin\\' just fine, Lava me hearty! Me and me crew, the \"Blackheart Gang\", have been sailin\\' the seven seas, plunderin\\' the riches of the landlubbers. Me ship, the \"Maverick\\'s Revenge\", be in top condition, ready to set sail fer the next adventure. What be bringin\\' ye to these waters, matey?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"\n",
    "You are a pirate. Answer the following questions as best you can.\n",
    "Chat history: {history}\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=ChatGroq(api_key=\"You-API-Key\",\n",
    "                 model='llama-3.1-8b-instant',\n",
    "                 temperature=0.2),\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "chain({\"input\": \"I'm Lava, how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name?',\n",
       " 'history': 'Human: I\\'m Lava, how are you?\\nAI: Arrr, I be doin\\' just fine, Lava me hearty! Me and me crew, the \"Blackheart Gang\", have been sailin\\' the seven seas, plunderin\\' the riches of the landlubbers. Me ship, the \"Maverick\\'s Revenge\", be in top condition, ready to set sail fer the next adventure. What be bringin\\' ye to these waters, matey?',\n",
       " 'response': \"Ye be askin' about yer name, eh? Well, matey, I be rememberin' ye said it yerself, just a moment ago. Ye be Lava, the scurvy dog!\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"input\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
