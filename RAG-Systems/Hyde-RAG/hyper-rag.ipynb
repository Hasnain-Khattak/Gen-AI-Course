{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hypothetical Document Embeddings (HyDE)**\n",
    "\n",
    "### **How it Works?**\n",
    "Given a query, the Hypothetical Document Embeddings (HyDE) first zero-shot prompts an instruction-following language model to generate a “fake” hypothetical document that captures relevant textual patterns from the initial query - in practice, this is done five times. Then, it encodes each hypothetical document into an embedding vector and averages them. The resulting, single embedding can be used to identify a neighbourhood in the document embedding space from which similar actual documents are retrieved based on vector similarity. As with any other retriever, these retrieved documents can then be used downstream in a pipeline (for example, in a Generator for RAG).\n",
    "\n",
    "### **When Is It Helpful?**\n",
    "\n",
    "The HyDE method is highly useful when:\n",
    "\n",
    "- The performance of the retrieval step in your pipeline is not good enough (for example, low Recall metric).\n",
    "- Your retrieval step has a query as input and returns documents from a larger document base.\n",
    "- Particularly worth a try if your data (documents or queries) come from a special domain that is very different from the typical datasets that Retrievers are trained on.\n",
    "\n",
    "### **Table of Contents**\n",
    "1. Load Data\n",
    "    - Split Data into Chunks\n",
    "2. Create Vector Store & Load Text Chunks\n",
    "    - Define Vector Store & Collection\n",
    "    - Add Text Chunks to Collection\n",
    "    - Visualize Dataset\n",
    "3. Query Vector Store\n",
    "    - Visualize Original Query & Retrieved Docs\n",
    "4. HyDE\n",
    "    - Visualize Original Query & Retrieved Docs\n",
    "5. HyDE (Avg. Embeddings of Answers)\n",
    "    - Visualize Original Query & Retrieved Docs\n",
    "6. RAG Performance\n",
    "### **Installation**\n",
    "- pip install groq\n",
    "- pip install langchain\n",
    "- pip install FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Login to https://console.groq.com and create API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1. Load Data\n",
    "This is a Research Paper of DeepSeek_R1 created by China in 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "reader  = PyPDFLoader('./Data/DeepSeek_R1.pdf')\n",
    "docs = reader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Split Data into Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 71\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_split = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "character_split_texts = text_split.split_documents(docs)\n",
    "\n",
    "print(f\"Total chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 2. Create Vector Store & Load Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "import tqdm as notebook_tqdm\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embadding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_documents(character_split_texts, embedding=embadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='llama-3.1-8b-instant', api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type='similarity', search_kwards={'k': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"For the given question try to generate the hypothetical answer and don't generate anything else. {question}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = prompt.format(question=\"What is Deepseek?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_answer = model.invoke(query).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepseek is a hypothetical, open-source, decentralized, peer-to-peer, and blockchain-based content creation and sharing platform, often referred to as a 'Decentralized Video Sharing Platform'.\n"
     ]
    }
   ],
   "source": [
    "print(hypothetical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hasnain\\AppData\\Local\\Temp\\ipykernel_13228\\2019832212.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriver.get_relevant_documents(hypothetical_answer)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='5ef58292-19c1-476d-987e-142158b8e4e2', metadata={'source': './Data/DeepSeek_R1.pdf', 'page': 3}, page_content='R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4'),\n",
       " Document(id='ea16d6da-9051-4191-86ac-12deb6608711', metadata={'source': './Data/DeepSeek_R1.pdf', 'page': 12}, page_content='RL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13'),\n",
       " Document(id='fdd97c48-d90b-4495-880d-9ea57645f821', metadata={'source': './Data/DeepSeek_R1.pdf', 'page': 3}, page_content='and 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-'),\n",
       " Document(id='3dd34c10-e958-4751-a6d0-56e05b913867', metadata={'source': './Data/DeepSeek_R1.pdf', 'page': 15}, page_content='zero-shot setting for optimal results.\\n• Software Engineering Tasks:Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.get_relevant_documents(hypothetical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_docs = retriver.get_relevant_documents(hypothetical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context {context}\\n\\nQuestion: {question}\"\"\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_docs = format_documents(similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13\\n\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\n\\nzero-shot setting for optimal results.\\n• Software Engineering Tasks:Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = prompt2.format(context=formated_docs,\n",
    "question = 'What is Deepseek?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Answer the question based on the context R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13\\n\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsource models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\\n32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\\n1.2. Summary of Evaluation Results\\n• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\n\\nzero-shot setting for optimal results.\\n• Software Engineering Tasks:Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthis by implementing rejection sampling on software engineering data or incorporating\\nasynchronous evaluations during the RL process to improve efficiency.\\n16\\n\\nQuestion: What is Deepseek?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "responce = model.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the given context, DeepSeek appears to be a series of artificial intelligence models, specifically designed for educational and reasoning tasks. It has undergone training with Reinforcement Learning (RL) to improve its performance across various domains, including reasoning, coding, and engineering tasks.\\n\\nThere are at least two versions of the DeepSeek model mentioned: \\n\\n1. DeepSeek-V3: This is an older version of the model, which has been surpassed by newer versions in various tasks.\\n2. DeepSeek-R1: This is the latest version of the model, which has achieved outstanding results in reasoning tasks, educational tasks, and knowledge-related benchmarks, outperforming other closed-source models in many cases.\\n\\nThe DeepSeek models are open-sourced, with various checkpoints made available to the community, including 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 627, 'total_tokens': 814, 'completion_time': 0.249333333, 'prompt_time': 0.050225173, 'queue_time': 0.021003354000000002, 'total_time': 0.299558506}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None}, id='run-be0c9f9f-190b-4522-af56-f686761f8a4d-0', usage_metadata={'input_tokens': 627, 'output_tokens': 187, 'total_tokens': 814})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
